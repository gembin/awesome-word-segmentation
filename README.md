# awesome-word-segmentation
[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

A curated list of resources dedicated to word segmentation

## Contents

### Chinese Word Segmentation Packages

1. [Jieba 结巴中文分词](https://github.com/fxsjy/jieba)
1. [SnowNLP: Simplified Chinese Text Processing](https://github.com/isnowfy/snownlp) 提供中文分词功能，分词是基于 [Character-Based Generative Model](http://aclweb.org/anthology//Y/Y09/Y09-2047.pdf) 实现
1. [THULAC (THU Lexical Analyzer for Chinese)](https://github.com/thunlp/THULAC-Python) 由清华大学自然语言处理与社会人文计算实验室研制推出的一套中文词法分析工具包，具有中文分词和词性标注功能
1. [kcws 深度学习中文分词](https://github.com/koth/kcws) BiLSTM+CRF 与 IDCNN+CRF
1. [FoolNLTK](https://github.com/rockyzhengwu/FoolNLTK) 可能不是最快的开源中文分词，但很可能是最准的开源中文分词
1. [ID-CNN-CWS](https://github.com/hankcs/ID-CNN-CWS) Iterated Dilated Convolutions for Chinese Word Segmentation
1. [Genius 中文分词](https://github.com/duanhongyi/genius) 采用 CRF(Conditional Random Field)条件随机场算法
1. [loso 中文分词](https://github.com/fangpenlin/loso)
1. [yaha "哑哈"中文分词](https://github.com/jannson/yaha)
1. [ChineseWordSegmentation](https://github.com/Moonshile/ChineseWordSegmentation) Chinese word segmentation algorithm without corpus（无需语料库的中文分词）
1. [pyltp](https://github.com/HIT-SCIR/pyltp) 是[语言技术平台（Language Technology Platform, LTP）](https://github.com/HIT-SCIR/ltp)的 Python 封装

