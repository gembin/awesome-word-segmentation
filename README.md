# awesome-word-segmentation
[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

A curated list of resources dedicated to word segmentation

## Contents

### Chinese Word Segmentation Packages

1. [Jieba 结巴中文分词](https://github.com/fxsjy/jieba)
1. [HanLP: Han Language Processing](https://github.com/hankcs/HanLP) 是由一系列模型与算法组成的NLP工具包，目标是普及自然语言处理在生产环境中的应用。[pyhanlp](https://github.com/hankcs/pyhanlp) 是 HanLP 的 Python 接口。 
1. [SnowNLP: Simplified Chinese Text Processing](https://github.com/isnowfy/snownlp) 提供中文分词功能，分词是基于 [Character-Based Generative Model](http://aclweb.org/anthology//Y/Y09/Y09-2047.pdf) 实现。
1. [Stanford CoreNLP](https://github.com/stanfordnlp/CoreNLP) is a Java suite of core NLP tools. 它提供分词功能。[stanford-corenlp](https://github.com/Lynten/stanford-corenlp) is a python wrapper for Stanford CoreNLP.
1. [THULAC (THU Lexical Analyzer for Chinese)](https://github.com/thunlp/THULAC-Python) 由清华大学自然语言处理与社会人文计算实验室研制推出的一套中文词法分析工具包，具有中文分词和词性标注功能
1. [kcws 深度学习中文分词](https://github.com/koth/kcws) BiLSTM+CRF 与 IDCNN+CRF
1. [FoolNLTK](https://github.com/rockyzhengwu/FoolNLTK) 可能不是最快的开源中文分词，但很可能是最准的开源中文分词
1. [ID-CNN-CWS](https://github.com/hankcs/ID-CNN-CWS) Iterated Dilated Convolutions for Chinese Word Segmentation
1. [Genius 中文分词](https://github.com/duanhongyi/genius) 采用 CRF(Conditional Random Field)条件随机场算法
1. [loso 中文分词](https://github.com/fangpenlin/loso)
1. [yaha "哑哈"中文分词](https://github.com/jannson/yaha)
1. [ChineseWordSegmentation](https://github.com/Moonshile/ChineseWordSegmentation) Chinese word segmentation algorithm without corpus（无需语料库的中文分词）
1. 语言技术平台[LTP: Language Technology Platform](https://github.com/HIT-SCIR/ltp) 提供分词功能；[pyltp](https://github.com/HIT-SCIR/pyltp) 是该平台的 Python 封装
1. [ICTCLAS](https://github.com/NLPIR-team/NLPIR-ICTCLAS) 分词工具。

